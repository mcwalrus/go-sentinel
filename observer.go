// Package sentinel provides reliability handling and observability monitoring for Go applications.
// It wraps task execution with Prometheus metrics, observing errors, panic occurrences, retries,
// and timeouts â€” making critical routines safe, measurable, and reliable.
package sentinel

import (
	"context"
	"errors"
	"time"

	"github.com/prometheus/client_golang/prometheus"
)

type observerConfig struct {
	namespace       string
	subsystem       string
	description     string
	bucketDurations []float64
	constLabels     prometheus.Labels
	taskConfig      *TaskConfig
}

// ObserverOption defines an configuration option for an Observer.
// Options are provided to [NewObserver] on setting up the Observer.
type ObserverOption func(*observerConfig)

// WithNamespace sets the Prometheus namespace for all metrics generated by this observer.
// For example, "myapp" results in metrics like "myapp_successes_total".
// If empty with subsystem, no namespace prefix is used.
func WithNamespace(namespace string) ObserverOption {
	return func(cfg *observerConfig) {
		cfg.namespace = namespace
	}
}

// WithSubsystem sets the Prometheus subsystem for all metrics generated by this observer.
// For example, "workers" results in metrics like "workers_successes_total".
// If empty with namespace, defaults to "sentinel" if not specified.
func WithSubsystem(subsystem string) ObserverOption {
	return func(cfg *observerConfig) {
		cfg.subsystem = subsystem
	}
}

// WithDescription sets the description used in the help text for generated metrics to
// describe what type of tasks are being observed. For example, "background processes"
// results in help text like: "Number of successes from observed background processes".
// Defaults to "tasks" if not specified.
func WithDescription(description string) ObserverOption {
	return func(cfg *observerConfig) {
		cfg.description = description
	}
}

// WithBucketDurations sets histogram bucket durations for the Observer.
// Each recorded task duration is assigned within one of the specified buckets.
// If not specified, no histogram metrics will be recorded or presented with the Observer.
// Please refer to [prometheus.HistogramOpts].Buckets for more information.
func WithBucketDurations(buckets []float64) ObserverOption {
	return func(cfg *observerConfig) {
		cfg.bucketDurations = buckets
	}
}

// WithConstLabels sets constant labels to exported metrics from the Observer.
// By default, no labels are attached. Please refer to [prometheus.Opts].ConstLabels
// for more information.
func WithConstLabels(labels prometheus.Labels) ObserverOption {
	return func(cfg *observerConfig) {
		cfg.constLabels = labels
	}
}

// WithDefaultTaskConfig sets the default [TaskConfig] for calls to [Observer.RunFunc].
// If not specified, a basic task configuration of panic recovery without retry attempts
// will be used.
func WithDefaultTaskConfig(taskConfig *TaskConfig) ObserverOption {
	return func(cfg *observerConfig) {
		cfg.taskConfig = taskConfig
	}
}

// Observer monitors and measures task executions, collecting Prometheus metrics
// for successes, failures, timeouts, panics, retries, and execution durations.
// It provides methods to execute tasks with various configuration options.
type Observer struct {
	cfg     observerConfig
	metrics *metrics
}

// NewObserver creates a new Observer instance with the specified options.
// The Observer will need to be registered with a Prometheus registry to expose metrics.
// Please refer to [Observer.MustRegister] and [Observer.Register] for more information.
//
// Example usage:
//
//	observer := sentinel.NewObserver() // Default configuration
//	observer := sentinel.NewObserver(sentinel.WithNamespace("myapp"))
//	observer := sentinel.NewObserver(
//	    sentinel.WithNamespace("myapp"),
//	    sentinel.WithSubsystem("workers"),
//	)
func NewObserver(opts ...ObserverOption) *Observer {
	cfg := observerConfig{
		namespace:   "",
		subsystem:   "",
		description: "tasks",
	}

	// Apply options
	for _, opt := range opts {
		opt(&cfg)
	}

	// Apply defaults if not set
	if cfg.subsystem == "" && cfg.namespace == "" {
		cfg.subsystem = "sentinel"
	}
	if cfg.description == "" {
		cfg.description = "tasks"
	}
	if cfg.taskConfig == nil {
		cfg.taskConfig = &TaskConfig{
			RecoverPanics: true,
		}
	}

	return &Observer{
		cfg:     cfg,
		metrics: newMetrics(cfg),
	}
}

// Register registers all Observer metrics with the provided Prometheus registry.
// Use [Observer.MustRegister] if you want the program to panic on registration conflicts.
func (o *Observer) Register(registry prometheus.Registerer) error {
	return o.metrics.Register(registry)
}

// MustRegister registers all Observer metrics with the provided Prometheus registry.
// This method panics if any metric registration failures. Use [Observer.Register] if you prefer
// to handle registration errors gracefully.
func (o *Observer) MustRegister(registry prometheus.Registerer) {
	o.metrics.MustRegister(registry)
}

// Run executes a function with the specified task configuration and observes its execution.
// All execution metrics (duration, success/failure, retries, etc) will be automatically recorded.
// If a [context.DeadlineExceeded] error is returne by fn(), it is recorded as a timeout error.
// When cfg.RecoverPanics is true, returns ErrPanicOccurred if a panic occurs and is recovered.
func (o *Observer) Run(cfg TaskConfig, fn func(ctx context.Context) error) error {
	if o == nil || o.metrics == nil {
		panic("observer: not configured")
	}
	task := &implTask{
		cfg: cfg,
		fn:  fn,
	}
	return o.observe(task)
}

// RunFunc executes a function with the DefaultTaskConfig and observes its execution.
// All execution metrics (duration, success/failure, panic, etc) will be automatically recorded.
// If a [context.DeadlineExceeded] error is returned, it is recorded as a timeout error.
// When RecoverPanics is true, returns ErrPanicOccurred if a panic occurs and is recovered.
func (o *Observer) RunFunc(fn func() error) error {
	if o == nil || o.metrics == nil {
		panic("observer: not configured")
	}
	task := &implTask{
		cfg: *o.cfg.taskConfig,
		fn: func(ctx context.Context) error {
			return fn() // ignore ctx
		},
	}
	return o.observe(task)
}

// RunTask executes a [Task] implementation and observes its execution.
// All execution metrics (duration, success/failure, retries, etc) will be automatically recorded.
// The task.Config() method determines the execution behaviour (timeout, retries, concurrency, etc).
// When RecoverPanics is true, returns ErrPanicOccurred if a panic occurs and is recovered.
func (o *Observer) RunTask(task Task) error {
	if o == nil || o.metrics == nil {
		panic("observer: not configured")
	}
	t := &implTask{
		cfg: task.Config(),
		fn:  task.Execute,
	}
	return o.observe(t)
}

// observe is the internal method that observes task execution.
// This method manages the InFlight gauge for individual tasks.
// Task execution and retries are handled by [Observer.executeTask].
func (o *Observer) observe(task *implTask) (err error) {
	o.metrics.InFlight.Inc()
	defer o.metrics.InFlight.Dec()
	return o.executeTask(task)
}

// executeTask performs the task execution logic and handles retries.
// Retry attempts are call method recursively for synchronous task handling.
// If panic occurs, the error from task.Execute() is switched for ErrPanicOccurred.
// This follows the behaviour defined by the [TaskConfig] task.Config() and [Task.Execute].
func (o *Observer) executeTask(task *implTask) error {
	var ctx = context.Background()
	if task.Config().Timeout > 0 {
		var cancel context.CancelFunc
		ctx, cancel = context.WithTimeout(ctx, task.Config().Timeout)
		defer cancel()
	}

	// Run task in closure to capture the panic
	var panicValue any
	err := func() (err error) {
		start := time.Now()
		defer func() {
			o.observeRuntime(start)
			if r := recover(); r != nil {
				panicValue = r
				err = &ErrPanicOccurred{panic: r}
			}
		}()
		err = task.Execute(ctx)
		return err
	}()

	// Handle errors
	if err != nil {
		o.metrics.Errors.Inc()
		if errors.Is(err, context.DeadlineExceeded) {
			o.metrics.TimeoutErrors.Inc()
		}

		// Handle panics
		if panicValue != nil {
			o.metrics.Panics.Inc()
			if !task.Config().RecoverPanics {
				panic(panicValue) // re-throw panic
			}
		}

		// Handle retries
		if task.Config().MaxRetries > 0 {
			cfg := task.Config()

			// Maximum retries reached
			if task.retryCount >= cfg.MaxRetries {
				return err
			} else {
				o.metrics.Retries.Inc()
			}

			// Try circuit break
			if cfg.RetryCircuitBreaker != nil {
				if cfg.RetryCircuitBreaker(err) {
					return err
				}
			}
			// Wait retry duration
			if cfg.RetryStrategy != nil {
				wait := cfg.RetryStrategy(task.retryCount)
				if wait > 0 {
					time.Sleep(wait)
				}
			}

			// Next retry attempt
			retryTask := &implTask{
				fn:         task.Execute,
				cfg:        cfg,
				retryCount: task.retryCount + 1,
			}

			// Run retry task
			err2 := o.observe(retryTask)
			if err2 != nil {
				return errors.Join(err, err2)
			} else {
				return nil // successful recursive return
			}
		}
	} else {
		o.metrics.Successes.Inc()
	}

	return err
}

// observeRuntime observes the runtime duration of the task execution.
func (o *Observer) observeRuntime(start time.Time) {
	if o.metrics.ObservedRuntimes != nil {
		o.metrics.ObservedRuntimes.Observe(time.Since(start).Seconds())
	}
}
